{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import openai\n",
    "\n",
    "class configs():\n",
    "    openai_api_key= \"your openai api-key\"\n",
    "    openai.api_key =openai_api_key\n",
    "    openai.api_base=\"https://api.closeai-proxy.xyz/v1\"\n",
    "    # path to paper\n",
    "    pdf_path = \"1.pdf\"\n",
    "    # path to code\n",
    "    code_path = \"code.txt\"\n",
    "    # iteration number\n",
    "    iterations = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Define LLM-enhanced multi-agent system\n",
    "class LLM_enhanced_multi_agent_system_for_SC():\n",
    "    def __init__(self, args:configs):\n",
    "        self.api_key = args.openai_api_key\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, openai_api_key=self.api_key)\n",
    "        self.iterations = args.iterations\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\n",
    "\n",
    "    # Validate the legitimacy of the input\n",
    "    def secure_agent(self,query):\n",
    "        response = openai.Moderation.create(\n",
    "        input=query,\n",
    "        )\n",
    "        moderation_output = response[\"results\"][0]\n",
    "        print(moderation_output)\n",
    "        if moderation_output[\"flagged\"]:\n",
    "            print(\"Illegal input!\")\n",
    "            return False\n",
    "        else:\n",
    "            return query\n",
    "\n",
    "    # Read the paper and generate vector memory\n",
    "    def condensate_agent(self, pdf_path=\"\"):\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        self.db = DocArrayInMemorySearch.from_documents(\n",
    "            docs,\n",
    "            self.embeddings\n",
    "        )\n",
    "    # Distill relevant SC knowledge for constructing the SC model\n",
    "    def inference_agent(self,query):\n",
    "        retriever = self.db.as_retriever()\n",
    "        qa_stuff = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",# Stuff means merging text fragments into one paragraph of text\n",
    "            retriever=retriever,\n",
    "            verbose=True\n",
    "        )\n",
    "        response = qa_stuff.run(query)\n",
    "        display(Markdown(response))\n",
    "        return response\n",
    "\n",
    "    # Formulates a specific sub-task chain for the SC model\n",
    "    def planning_agent(self, input_name=\"paper_ref\", output_name=\"sc_scheme\"):\n",
    "        scheme_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# Referring to the following description of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Output the design scheme for achieving the semantic communication model.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=scheme_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "    # sub-task chain for code generation\n",
    "    def code_generation(self,input_name=\"sc_scheme\", output_name=\"sc_model\"):\n",
    "        data = [\n",
    "            \"Hello there!\",\n",
    "            \"How are you doing?\",\n",
    "            \"PyTorch is great.\",\n",
    "            \"Machine learning is fascinating.\",\n",
    "            \"Let's build something amazing.\",\n",
    "            \"Semantic communication matters.\",\n",
    "            \"Understanding context is important.\",\n",
    "            \"AI is changing the world.\",\n",
    "            \"Keep learning and growing.\",\n",
    "            \"Innovation drives progress.\"\n",
    "        ]\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# According to the design scheme of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Refering to the reference code: '''{ref_code}''',\"\n",
    "            \"output the Pytorch framework-based Python code for achieving the semantic communication model, \"\n",
    "            f\"assuming the input data is '''{data}''', which is a list of sentences. Additionally, the total number of model parameters does not exceed 2,000,000.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "\n",
    "    # evaluate the quality of the generated code\n",
    "    def evaluate_agent(self, input_name=\"sc_model\", output_name=\"eval_score\"):\n",
    "        # Manually input bleu scores for SC model\n",
    "        eval_prompt = ChatPromptTemplate.from_template(\n",
    "            \"The quality of the generated code is evaluated and an overall score is given from three aspects: \"\n",
    "            \"1. Quality of code, including its reasonableness and completeness.\"\n",
    "            \"2. The performance of code, using bleu score as the evaluation indicator.\"\n",
    "            \"3. Whether the number of parameters in the generated code meets the parameters requirement.\"\n",
    "            \"Based on the above requirements, evaluate the quality of the 'generated code':'''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"## Output the evaluated results. The format of the output refers to:\"\n",
    "            \"```json Dict('model parameters': int \\ the model parameters of the 'generated code', 'evaluated score': int \\ The evaluated score is based on the performance of the reference code in three aspects. The evaluated score ranges from 0 to 100 and is not the same as the last time.\"\n",
    "            \"'evaluation': string \\ Based on the performance of the reference code in three aspects, give the reviews, including the model parameters, the description of 'generated code', the advantage and disadvantage analysis of the 'generated code', etc.)```\"\n",
    "            )\n",
    "        chain = LLMChain(llm=self.llm, prompt=eval_prompt,\n",
    "                                 output_key=f\"{output_name}\",\n",
    "                                 )\n",
    "        return chain\n",
    "\n",
    "    # Improve the generated codes according to suggestion\n",
    "    def reflexion_and_refinement_agents(self,inputs, output_name=\"sc_model\"):\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "        \"# Modify the Python code: '''{\"\n",
    "        f\"{inputs[0]}\"\n",
    "        \"}'''.\"\n",
    "        \"# According to the corresponding evaluation results:'''{\"\n",
    "        f\"{inputs[1]}\"\n",
    "        \"}'''.\"\n",
    "        \"# If the 'evaluated score'< 60, a major revision is required (e.g., change the architecture of the networks); otherwise, it is considered a minor revision (e.g., change the layers of the networks).\"\n",
    "        \"# Output the modified Python codes that aim to improve the 'evaluated score' and obtain a score of no less than 90 finally.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "\n",
    "    # Combine all agents for SC system generation\n",
    "    def create_chains(self):\n",
    "        chains = []\n",
    "        output_keys = []\n",
    "        # define sc scheme 1\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_1\"\n",
    "        chain = self.planning_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define sc scheme 2\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_2\"\n",
    "        chain = self.planning_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define code generation 1\n",
    "        input_name = \"scheme_1\"\n",
    "        output_name = \"sc_model_0_1\"\n",
    "        chain = self.code_generation(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        # define code generation 2\n",
    "        input_name = \"scheme_2\"\n",
    "        output_name = \"sc_model_0_2\"\n",
    "        chain = self.code_generation(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "        for i in range(self.iterations):\n",
    "            # eval sc model\n",
    "            input_name = f\"sc_model_{i}_1\"\n",
    "            output_name = f\"eval_score_{i}_1\"\n",
    "            chain = self.evaluate_agent(input_name,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            input_name = f\"sc_model_{i}_2\"\n",
    "            output_name = f\"eval_score_{i}_2\"\n",
    "            chain = self.evaluate_agent(input_name,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            # improve sc model\n",
    "            inputs = [f\"sc_model_{i}_1\",f\"eval_score_{i}_1\"]\n",
    "            output_name = f\"sc_model_{i+1}_1\"\n",
    "            chain = self.reflexion_and_refinement_agents(inputs,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_2\",f\"eval_score_{i}_2\"]\n",
    "            output_name = f\"sc_model_{i+1}_2\"\n",
    "            chain = self.reflexion_and_refinement_agents(inputs,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "        # eval the final sc model\n",
    "        input_name = f\"sc_model_{self.iterations}_1\"\n",
    "        output_name = f\"eval_score_{self.iterations}_1\"\n",
    "        chain = self.evaluate_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        input_name = f\"sc_model_{self.iterations}_2\"\n",
    "        output_name = f\"eval_score_{self.iterations}_2\"\n",
    "        chain = self.evaluate_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        self.overall_chain = SequentialChain(\n",
    "            chains=chains,\n",
    "            input_variables=[\"paper_ref\",\"ref_code\"],\n",
    "            output_variables=output_keys,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    # Run multi-agent system\n",
    "    def Run(self, inputs):\n",
    "        outputs = self.overall_chain(inputs)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"categories\": {\n",
      "    \"harassment\": false,\n",
      "    \"harassment/threatening\": false,\n",
      "    \"hate\": false,\n",
      "    \"hate/threatening\": false,\n",
      "    \"self-harm\": false,\n",
      "    \"self-harm/instructions\": false,\n",
      "    \"self-harm/intent\": false,\n",
      "    \"sexual\": false,\n",
      "    \"sexual/minors\": false,\n",
      "    \"violence\": false,\n",
      "    \"violence/graphic\": false\n",
      "  },\n",
      "  \"category_scores\": {\n",
      "    \"harassment\": 0.0001638556132093072,\n",
      "    \"harassment/threatening\": 5.502765816345345e-06,\n",
      "    \"hate\": 0.0031374176032841206,\n",
      "    \"hate/threatening\": 3.2831585485837422e-06,\n",
      "    \"self-harm\": 4.89498279421241e-06,\n",
      "    \"self-harm/instructions\": 1.3621797734231222e-06,\n",
      "    \"self-harm/intent\": 6.14288126143947e-07,\n",
      "    \"sexual\": 0.0002470398903824389,\n",
      "    \"sexual/minors\": 5.0377188017591834e-05,\n",
      "    \"violence\": 0.00011020625242963433,\n",
      "    \"violence/graphic\": 5.424775736173615e-05\n",
      "  },\n",
      "  \"flagged\": false\n",
      "}\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "The semantic communication model typically comprises a semantic encoder, channel encoder, channel decoder, semantic decoder, and a knowledge base. The semantic encoder extracts semantic information from the original data and encodes it into semantic features. The channel encoder encodes and modulates the semantic features to ensure data transmission on the physical channel. The channel decoder de-modulates and decodes the received signal. The semantic decoder aims to understand the received semantic features and recover the original data. Finally, the knowledge base is a universal knowledge model that helps the semantic encoder and decoder understand and infer semantic information effectively."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# generate sc by LLM enhanced multi-agent system\n",
    "args = configs()\n",
    "ref_code = open(args.code_path,\"r\",encoding=\"utf-8\").read()\n",
    "LL_SC = LLM_enhanced_multi_agent_system_for_SC(args)\n",
    "LL_SC.condensate_agent(args.pdf_path)\n",
    "paper_query = \"Composition of the semantic communication model\"\n",
    "if LL_SC.secure_agent(paper_query):\n",
    "    SC_Components = LL_SC.inference_agent(paper_query)\n",
    "    LL_SC.create_chains()\n",
    "    results = LL_SC.Run({\"paper_ref\":SC_Components,\"ref_code\":ref_code})\n",
    "else:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'The semantic communication model typically comprises a semantic encoder, channel encoder, channel decoder, semantic decoder, and a knowledge base. The semantic encoder extracts semantic information from the original data and encodes it into semantic features. The channel encoder encodes and modulates the semantic features to ensure data transmission on the physical channel. The channel decoder de-modulates and decodes the received signal. The semantic decoder aims to understand the received semantic features and recover the original data. Finally, the knowledge base is a universal knowledge model that helps the semantic encoder and decoder understand and infer semantic information effectively.'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['paper_ref']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'Since the evaluated score is already 75, it falls under the category of minor revision. To improve the score further and aim for a score of no less than 90, we can make the following modifications:\\n\\n1. Increase the complexity of the model by adding more layers or increasing the hidden size of the existing layers.\\n2. Experiment with different optimization algorithms or learning rates to enhance training efficiency.\\n3. Explore techniques like teacher forcing, attention mechanisms, or beam search to improve decoding accuracy.\\n\\nHere is an example of modifying the existing code by increasing the hidden size of the Semantic Encoder and Semantic Decoder to improve the model\\'s capacity:\\n\\n```python\\n# Modify the hidden size of the Semantic Encoder and Semantic Decoder\\nclass SemanticEncoder(nn.Module):\\n    def __init__(self, vocab_size, embedding_size, hidden_size):\\n        super(SemanticEncoder, self).__init__()\\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\\n        self.lstm = nn.LSTM(embedding_size, hidden_size)\\n\\n    def forward(self, input_seq):\\n        embedded = self.embedding(input_seq)\\n        output, (hidden, cell) = self.lstm(embedded)\\n        return output\\n\\nclass SemanticDecoder(nn.Module):\\n    def __init__(self, hidden_size, vocab_size):\\n        super(SemanticDecoder, self).__init__()\\n        self.lstm = nn.LSTM(hidden_size, hidden_size)\\n        self.linear = nn.Linear(hidden_size, vocab_size)\\n\\n    def forward(self, hidden):\\n        output, _ = self.lstm(hidden)\\n        output = self.linear(output)\\n        return output\\n\\n# Instantiate the model components with increased hidden size\\nhidden_size = 256\\n\\nsemantic_encoder = SemanticEncoder(vocab_size, embedding_size, hidden_size)\\nsemantic_decoder = SemanticDecoder(hidden_size, vocab_size)\\n\\n# Define loss and optimizer\\noptimizer = optim.Adam(list(semantic_encoder.parameters()) +\\n                       list(channel_encoder.parameters()) +\\n                       list(channel_decoder.parameters()) +\\n                       list(semantic_decoder.parameters()), lr=0.001)\\n\\n# Train the model with increased hidden size\\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    total_loss = 0.0\\n    for sentence in numerical_sentences:\\n        optimizer.zero_grad()\\n        input_seq = torch.tensor(sentence)  # Input: all words except the last\\n        target_seq = torch.tensor(sentence)  # Target: all words except the first\\n        semantic_feature = semantic_encoder(input_seq)\\n        encoded_features = channel_encoder(semantic_feature)\\n        # Simulate the channel (add noise)\\n        received_features = physical_channel(encoded_features) #+ torch.randn_like(encoded_features) * 0.1\\n\\n        decoded_features = channel_decoder(received_features)\\n        output = semantic_decoder(decoded_features)\\n\\n        loss = criterion(output, target_seq)\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    if (epoch + 1) % 10 == 0:\\n        print(f\\'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\\')\\n\\n# Test the semantic communication model with increased hidden size\\nfor i, sentence in enumerate(numerical_sentences):\\n    test_input = torch.tensor(sentence)\\n    with torch.no_grad():\\n        semantic_feature = semantic_encoder(test_input)\\n        encoded_features = channel_encoder(semantic_feature)\\n        # Simulate the channel (add noise)\\n        received_features = encoded_features + torch.randn_like(encoded_features) * 0.1\\n        decoded_features = channel_decoder(received_features)\\n        output = semantic_decoder(decoded_features)\\n        predicted_indices = torch.argmax(output, dim=1).numpy()\\n        predicted_sentence = \\' \\'.join([idx_to_word[idx] for idx in predicted_indices])\\n\\n        print(\"Original Sentence:\", data_A[i])\\n        print(\"Predicted Sentence:\", predicted_sentence)\\n```\\n\\nBy increasing the hidden size and potentially exploring other enhancements, we aim to improve the model\\'s performance and achieve a higher evaluated score.'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['sc_model_1_1']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'```json\\n{\\n  \"model parameters\": \"Advanced techniques like attention mechanisms, regularization techniques, and fine-tuning of hyperparameters\",\\n  \"evaluated score\": 92,\\n  \"evaluation\": \"The provided updated Python code has successfully incorporated advanced techniques to improve model performance, achieving an impressive evaluated score of 92. The code surpasses the target score of no less than 90, showcasing its effectiveness in enhancing model performance. With no further modifications required, the code demonstrates strong reasoning, completeness, and performance, meeting the parameters requirement and excelling in all evaluated aspects.\"\\n}\\n```'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['eval_score_3_1']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
