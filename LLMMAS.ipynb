{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import openai\n",
    "\n",
    "class configs():\n",
    "    openai_api_key= \"your openai api-key\"\n",
    "    openai.api_key =openai_api_key\n",
    "    openai.api_base=\"https://api.closeai-proxy.xyz/v1\"\n",
    "    # path to the reference paper that describes the semantic communications\n",
    "    pdf_path = \"1.pdf\"\n",
    "    # path to the reference code that is as an example\n",
    "    code_path = \"code.txt\"\n",
    "    # iteration number\n",
    "    iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define LLM-enhanced multi-agent system\n",
    "class LLM_enhanced_multi_agent_system_for_SC():\n",
    "    def __init__(self, args: configs):\n",
    "        self.api_key = args.openai_api_key\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, openai_api_key=self.api_key)\n",
    "        self.iterations = args.iterations\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\n",
    "\n",
    "    # Validate the legitimacy of the input\n",
    "    def secure_agent(self, query):\n",
    "        response = openai.Moderation.create(\n",
    "            input=query,\n",
    "        )\n",
    "        moderation_output = response[\"results\"][0]\n",
    "        print(moderation_output)\n",
    "        if moderation_output[\"flagged\"]:\n",
    "            print(\"Illegal input!\")\n",
    "            return False\n",
    "        else:\n",
    "            return query\n",
    "\n",
    "    # Load the paper and generate vector memory, which is the knowledge base\n",
    "    def condensate_agent(self, pdf_path=\"\"):\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        # construct knowledge base\n",
    "        self.db = DocArrayInMemorySearch.from_documents(\n",
    "            docs,\n",
    "            self.embeddings\n",
    "        )\n",
    "\n",
    "    # Distill relevant SC knowledge for constructing the SC model\n",
    "    def inference_agent(self, query):\n",
    "        retriever = self.db.as_retriever()\n",
    "        qa_stuff = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",  # Stuff means merging text fragments into one paragraph of text\n",
    "            retriever=retriever,\n",
    "            verbose=True\n",
    "        )\n",
    "        response = qa_stuff.run(query)\n",
    "        display(Markdown(response))\n",
    "        return response\n",
    "\n",
    "    # Formulates a specific sub-task chain for the SC model\n",
    "    def planning_agent(self, input_name=\"paper_ref\", output_name=\"sc_scheme\"):\n",
    "        scheme_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# Referring to the following description of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Output the design scheme for achieving the semantic communication model.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=scheme_prompt,\n",
    "                         output_key=output_name,\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    # sub-task chain for code generation\n",
    "    def code_generation(self, input_name=\"sc_scheme\", output_name=\"sc_model\"):\n",
    "        data = [\n",
    "            \"Hello there!\",\n",
    "            \"How are you doing?\",\n",
    "            \"PyTorch is great.\",\n",
    "            \"Machine learning is fascinating.\",\n",
    "            \"Let's build something amazing.\",\n",
    "            \"Semantic communication matters.\",\n",
    "            \"Understanding context is important.\",\n",
    "            \"AI is changing the world.\",\n",
    "            \"Keep learning and growing.\",\n",
    "            \"Innovation drives progress.\"\n",
    "        ]\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# According to the design scheme of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Refering to the reference code: '''{ref_code}''',\"\n",
    "            \"output the Pytorch framework-based Python code for achieving the semantic communication model, \"\n",
    "            f\"assuming the input data is '''{data}''', which is a list of sentences. \"\n",
    "            f\"Specifically, the Additive Gaussian White Noise (AWGN) can serve as the physical channel. The Bilingual Evaluation Understudy (BLEU) score is adopted as the metric that evaluates the SC model. We expect that the generated SC model achieves no less than a 0.6 BLEU score when the Signal-to-Noise Ratio (SNR) is 10 dB. The total number of model parameters does not exceed 2,000,000 for the resource constraints of devices.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                         output_key=output_name,\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    # evaluate the quality of the generated code\n",
    "    def evaluate_agent(self, input_name=\"sc_model\", output_name=\"eval_score\"):\n",
    "        # Manually input bleu scores for SC model\n",
    "        eval_prompt = ChatPromptTemplate.from_template(\n",
    "            \"The quality of the generated code is evaluated and an overall score is given from three aspects: \"\n",
    "            \"1. Quality of code, including its reasonableness and completeness.\"\n",
    "            \"2. The performance of code, using bleu score as the evaluation indicator.\"\n",
    "            \"3. Whether the number of parameters in the generated code meets the limitation.\"\n",
    "            \"Based on the above requirements, evaluate the quality of the 'generated code':'''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"## Output the evaluated results. The format of the output refers to:\"\n",
    "            \"```json Dict('model parameters': int \\ the model parameters of the 'generated code', 'evaluated score': int \\ The evaluated score is based on the performance of the reference code in three aspects. The evaluated score ranges from 0 to 100 and is not the same as the last time.\"\n",
    "            \"'evaluation': string \\ Based on the performance of the reference code in three aspects, give the reviews, including the model parameters, the description of 'generated code', the advantage and disadvantage analysis of the 'generated code', etc.\"\n",
    "            \"There is an example: 'The Python-based SC model has been successfully implemented, incorporating all necessary modules. The semantic encoder and decoder are realized using Long Short-Term Memory (LSTM) networks. The channel encoder and decoder are constructed based on the Multilayer Perceptron (MLP) architecture.  The final SC model can achieve a 0.68 BLEU score when SNR is 10 dB, which meets expectations. In addition, the total number of model parameters is 1,826,762.')```\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=eval_prompt,\n",
    "                         output_key=f\"{output_name}\",\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    def long_memory_stroage(self, inputs, output_name=\"long_term_memory\"):\n",
    "        memory_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# According to the code and evaluation results of the current subtask chain:'''{\"\n",
    "            f\"{inputs[0]}\"\n",
    "            \"}''' and '''{\"\n",
    "            f\"{inputs[1]}\"\n",
    "            \"}'''.\"\n",
    "            \"# If there is a significant difference in the semantic space of the current subtask chain, i.e. the 'evaluation score' is less than 60.\"\n",
    "            \"# Then only the module composition of the sub task chain code needs to be output, such as LSTM, MLP, etc.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=memory_prompt,\n",
    "                         output_key=f\"{output_name}\",\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    def short_memory_stroage(self, inputs, output_name=\"short_term_memory\"):\n",
    "        memory_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# According to the code and evaluation results of the current subtask chain:'''{\"\n",
    "            f\"{inputs[0]}\"\n",
    "            \"}''' and '''{\"\n",
    "            f\"{inputs[1]}\"\n",
    "            \"}'''.\"\n",
    "            \"# If the current subtask chain has semantic similarity in the semantic space, i.e. the 'evaluation score' is greater than 60.\"\n",
    "            \"# Not only does it output the module composition of the subtask chain code, such as LSTM, MLP, etc., but it also outputs the evaluation results of the subtask chain\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=memory_prompt,\n",
    "                         output_key=f\"{output_name}\",\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    def reflexion_agent(self, inputs, output_name=\"sc_model\"):\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# Modify the Python code: '''{\"\n",
    "            f\"{inputs[0]}\"\n",
    "            \"}'''.\"\n",
    "            \"# According to the corresponding evaluation results:'''{\"\n",
    "            f\"{inputs[1]}\"\n",
    "            \"}'''.\"\n",
    "            \"# According to short-term memory:'''{\"\n",
    "            f\"{inputs[2]}\"\n",
    "            \"}'''.\"\n",
    "            \"# Extracting fine-grained information, similar to how humans can recall recent details, considering the performance of the current subtask chain in historical schemes, provides valuable small feedback for improving code.\"\n",
    "            \"# Output the modified Python codes that aim to improve the 'evaluated score' and obtain a score of no less than 90 finally.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                         output_key=output_name,\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    def refinement_agent(self, inputs, output_name=\"sc_model\"):\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# Modify the Python code: '''{\"\n",
    "            f\"{inputs[0]}\"\n",
    "            \"}'''.\"\n",
    "            \"# According to the corresponding evaluation results:'''{\"\n",
    "            f\"{inputs[1]}\"\n",
    "            \"}'''.\"\n",
    "            \"# According to long-term memory:'''{\"\n",
    "            f\"{inputs[2]}\"\n",
    "            \"}'''.\"\n",
    "            \"# Quoting coarse-grained information is similar to the way humans extract important experiences from long-term decisions, considering the performance of the current subtask chain from a global perspective and providing large-scale feedback for improving subtask chain code.\"\n",
    "            \"# Output the modified Python codes that aim to improve the 'evaluated score' and obtain a score of no less than 90 finally.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                         output_key=output_name,\n",
    "                         )\n",
    "        return chain\n",
    "\n",
    "    # Combine all agents for SC system generation\n",
    "    def create_chains(self):\n",
    "        chains = []\n",
    "        output_keys = []\n",
    "\n",
    "        # define sc scheme 1\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_1\"\n",
    "        chain = self.planning_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define sc scheme 2\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_2\"\n",
    "        chain = self.planning_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define code generation 1\n",
    "        input_name = \"scheme_1\"\n",
    "        output_name = \"sc_model_0_1\"\n",
    "        chain = self.code_generation(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        # define code generation 2\n",
    "        input_name = \"scheme_2\"\n",
    "        output_name = \"sc_model_0_2\"\n",
    "        chain = self.code_generation(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "        for i in range(self.iterations):\n",
    "            # eval sc model\n",
    "            input_name = f\"sc_model_{i}_1\"\n",
    "            output_name = f\"eval_score_{i}_1\"\n",
    "            chain = self.evaluate_agent(input_name, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            input_name = f\"sc_model_{i}_2\"\n",
    "            output_name = f\"eval_score_{i}_2\"\n",
    "            chain = self.evaluate_agent(input_name, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            # memory storage\n",
    "            inputs = [f\"sc_model_{i}_1\", f\"eval_score_{i}_1\"]\n",
    "            output_name = f\"long_term_memory_{i}_1\"\n",
    "            chain = self.long_memory_stroage(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_1\", f\"eval_score_{i}_1\"]\n",
    "            output_name = f\"short_term_memory_{i}_1\"\n",
    "            chain = self.short_memory_stroage(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_2\", f\"eval_score_{i}_2\"]\n",
    "            output_name = f\"long_term_memory_{i}_2\"\n",
    "            chain = self.long_memory_stroage(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_2\", f\"eval_score_{i}_2\"]\n",
    "            output_name = f\"short_term_memory_{i}_2\"\n",
    "            chain = self.short_memory_stroage(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            # Reflect and modify the SC model according to the evaluated results and memory\n",
    "            inputs = [f\"sc_model_{i}_1\", f\"eval_score_{i}_1\", f\"short_term_memory_{i}_1\"]\n",
    "            output_name = f\"sc_modelx_{i}_1\"\n",
    "            chain = self.reflexion_agent(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_2\", f\"eval_score_{i}_2\", f\"short_term_memory_{i}_2\"]\n",
    "            output_name = f\"sc_modelx_{i}_2\"\n",
    "            chain = self.reflexion_agent(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_modelx_{i}_1\", f\"eval_score_{i}_1\", f\"long_term_memory_{i}_1\"]\n",
    "            output_name = f\"sc_model_{i + 1}_1\"\n",
    "            chain = self.refinement_agent(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_modelx_{i}_2\", f\"eval_score_{i}_2\", f\"long_term_memory_{i}_2\"]\n",
    "            output_name = f\"sc_model_{i + 1}_2\"\n",
    "            chain = self.refinement_agent(inputs, output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "        # eval the final sc model\n",
    "        input_name = f\"sc_model_{self.iterations}_1\"\n",
    "        output_name = f\"eval_score_{self.iterations}_1\"\n",
    "        chain = self.evaluate_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        input_name = f\"sc_model_{self.iterations}_2\"\n",
    "        output_name = f\"eval_score_{self.iterations}_2\"\n",
    "        chain = self.evaluate_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        self.overall_chain = SequentialChain(\n",
    "            chains=chains,\n",
    "            input_variables=[\"paper_ref\", \"ref_code\"],\n",
    "            output_variables=output_keys,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    # Run multi-agent system\n",
    "    def Run(self, inputs):\n",
    "        outputs = self.overall_chain(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# generate sc by LLM enhanced multi-agent system\n",
    "args = configs()\n",
    "ref_code = open(args.code_path,\"r\",encoding=\"utf-8\").read()\n",
    "LL_SC = LLM_enhanced_multi_agent_system_for_SC(args)\n",
    "LL_SC.condensate_agent(args.pdf_path)\n",
    "paper_query = \"Composition of the semantic communication model\"\n",
    "if LL_SC.secure_agent(paper_query):\n",
    "    SC_Components = LL_SC.inference_agent(paper_query)\n",
    "    LL_SC.create_chains()\n",
    "    results = LL_SC.Run({\"paper_ref\":SC_Components,\"ref_code\":ref_code})\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
