{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import openai\n",
    "\n",
    "class configs():\n",
    "    openai_api_key= \"your openai api-key\"\n",
    "    openai.api_key =openai_api_key\n",
    "    openai.api_base=\"https://api.closeai-proxy.xyz/v1\"\n",
    "    # path to the reference paper that describes the semantic communications\n",
    "    pdf_path = \"1.pdf\"\n",
    "    # path to the reference code that is as an example\n",
    "    code_path = \"code.txt\"\n",
    "    # iteration number\n",
    "    iterations = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Define LLM-enhanced multi-agent system\n",
    "class LLM_enhanced_multi_agent_system_for_SC():\n",
    "    def __init__(self, args:configs):\n",
    "        self.api_key = args.openai_api_key\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, openai_api_key=self.api_key)\n",
    "        self.iterations = args.iterations\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\n",
    "\n",
    "    # Validate the legitimacy of the input\n",
    "    def secure_agent(self,query):\n",
    "        response = openai.Moderation.create(\n",
    "        input=query,\n",
    "        )\n",
    "        moderation_output = response[\"results\"][0]\n",
    "        print(moderation_output)\n",
    "        if moderation_output[\"flagged\"]:\n",
    "            print(\"Illegal input!\")\n",
    "            return False\n",
    "        else:\n",
    "            return query\n",
    "\n",
    "    # Load the paper and generate vector memory\n",
    "    def condensate_agent(self, pdf_path=\"\"):\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        self.db = DocArrayInMemorySearch.from_documents(\n",
    "            docs,\n",
    "            self.embeddings\n",
    "        )\n",
    "    # Distill relevant SC knowledge for constructing the SC model\n",
    "    def inference_agent(self,query):\n",
    "        retriever = self.db.as_retriever()\n",
    "        qa_stuff = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",# Stuff means merging text fragments into one paragraph of text\n",
    "            retriever=retriever,\n",
    "            verbose=True\n",
    "        )\n",
    "        response = qa_stuff.run(query)\n",
    "        display(Markdown(response))\n",
    "        return response\n",
    "\n",
    "    # Formulates a specific sub-task chain for the SC model\n",
    "    def planning_agent(self, input_name=\"paper_ref\", output_name=\"sc_scheme\"):\n",
    "        scheme_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# Referring to the following description of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Output the design scheme for achieving the semantic communication model.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=scheme_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "    # sub-task chain for code generation\n",
    "    def code_generation(self,input_name=\"sc_scheme\", output_name=\"sc_model\"):\n",
    "        data = [\n",
    "            \"Hello there!\",\n",
    "            \"How are you doing?\",\n",
    "            \"PyTorch is great.\",\n",
    "            \"Machine learning is fascinating.\",\n",
    "            \"Let's build something amazing.\",\n",
    "            \"Semantic communication matters.\",\n",
    "            \"Understanding context is important.\",\n",
    "            \"AI is changing the world.\",\n",
    "            \"Keep learning and growing.\",\n",
    "            \"Innovation drives progress.\"\n",
    "        ]\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "            \"# According to the design scheme of the semantic communication model: '''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"# Refering to the reference code: '''{ref_code}''',\"\n",
    "            \"output the Pytorch framework-based Python code for achieving the semantic communication model, \"\n",
    "            f\"assuming the input data is '''{data}''', which is a list of sentences. \"\n",
    "            f\"Specifically, the Additive Gaussian White Noise (AWGN) can serve as the physical channel. The Bilingual Evaluation Understudy (BLEU) score is adopted as the metric that evaluates the SC model. We expect that the generated SC model achieves no less than a 0.6 BLEU score when the Signal-to-Noise Ratio (SNR) is 10 dB. The total number of model parameters does not exceed 2,000,000 for the resource constraints of devices.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "\n",
    "    # evaluate the quality of the generated code\n",
    "    def evaluate_agent(self, input_name=\"sc_model\", output_name=\"eval_score\"):\n",
    "        # Manually input bleu scores for SC model\n",
    "        eval_prompt = ChatPromptTemplate.from_template(\n",
    "            \"The quality of the generated code is evaluated and an overall score is given from three aspects: \"\n",
    "            \"1. Quality of code, including its reasonableness and completeness.\"\n",
    "            \"2. The performance of code, using bleu score as the evaluation indicator.\"\n",
    "            \"3. Whether the number of parameters in the generated code meets the limitation.\"\n",
    "            \"Based on the above requirements, evaluate the quality of the 'generated code':'''{\"\n",
    "            f\"{input_name}\"\n",
    "            \"}'''.\"\n",
    "            \"## Output the evaluated results. The format of the output refers to:\"\n",
    "            \"```json Dict('model parameters': int \\ the model parameters of the 'generated code', 'evaluated score': int \\ The evaluated score is based on the performance of the reference code in three aspects. The evaluated score ranges from 0 to 100 and is not the same as the last time.\"\n",
    "            \"'evaluation': string \\ Based on the performance of the reference code in three aspects, give the reviews, including the model parameters, the description of 'generated code', the advantage and disadvantage analysis of the 'generated code', etc.\"\n",
    "            \"There is an example: 'The Python-based SC model has been successfully implemented, incorporating all necessary modules. The semantic encoder and decoder are realized using Long Short-Term Memory (LSTM) networks. The channel encoder and decoder are constructed based on the Multilayer Perceptron (MLP) architecture.  The final SC model can achieve a 0.68 BLEU score when SNR is 10 dB, which meets expectations. In addition, the total number of model parameters is 1,826,762.')```\"\n",
    "            )\n",
    "        chain = LLMChain(llm=self.llm, prompt=eval_prompt,\n",
    "                                 output_key=f\"{output_name}\",\n",
    "                                 )\n",
    "        return chain\n",
    "\n",
    "    # Improve the generated codes according to suggestion\n",
    "    def reflexion_and_refinement_agents(self,inputs, output_name=\"sc_model\"):\n",
    "        model_prompt = ChatPromptTemplate.from_template(\n",
    "        \"# Modify the Python code: '''{\"\n",
    "        f\"{inputs[0]}\"\n",
    "        \"}'''.\"\n",
    "        \"# According to the corresponding evaluation results:'''{\"\n",
    "        f\"{inputs[1]}\"\n",
    "        \"}'''.\"\n",
    "        \"# If the 'evaluated score'< 60, a major revision is required (e.g., change the architecture of the networks); otherwise, it is considered a minor revision (e.g., change the layers of the networks).\"\n",
    "        \"# Output the modified Python codes that aim to improve the 'evaluated score' and obtain a score of no less than 90 finally.\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=model_prompt,\n",
    "                             output_key=output_name,\n",
    "                             )\n",
    "        return chain\n",
    "\n",
    "    # Combine all agents for SC system generation\n",
    "    def create_chains(self):\n",
    "        chains = []\n",
    "        output_keys = []\n",
    "        # define sc scheme 1\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_1\"\n",
    "        chain = self.planning_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define sc scheme 2\n",
    "        input_name = \"paper_ref\"\n",
    "        output_name = \"scheme_2\"\n",
    "        chain = self.planning_agent(input_name, output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(output_name)\n",
    "        # define code generation 1\n",
    "        input_name = \"scheme_1\"\n",
    "        output_name = \"sc_model_0_1\"\n",
    "        chain = self.code_generation(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        # define code generation 2\n",
    "        input_name = \"scheme_2\"\n",
    "        output_name = \"sc_model_0_2\"\n",
    "        chain = self.code_generation(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "        for i in range(self.iterations):\n",
    "            # eval sc model\n",
    "            input_name = f\"sc_model_{i}_1\"\n",
    "            output_name = f\"eval_score_{i}_1\"\n",
    "            chain = self.evaluate_agent(input_name,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            input_name = f\"sc_model_{i}_2\"\n",
    "            output_name = f\"eval_score_{i}_2\"\n",
    "            chain = self.evaluate_agent(input_name,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            # improve sc model\n",
    "            inputs = [f\"sc_model_{i}_1\",f\"eval_score_{i}_1\"]\n",
    "            output_name = f\"sc_model_{i+1}_1\"\n",
    "            chain = self.reflexion_and_refinement_agents(inputs,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "            inputs = [f\"sc_model_{i}_2\",f\"eval_score_{i}_2\"]\n",
    "            output_name = f\"sc_model_{i+1}_2\"\n",
    "            chain = self.reflexion_and_refinement_agents(inputs,output_name)\n",
    "            chains.append(chain)\n",
    "            output_keys.append(chain.output_key)\n",
    "\n",
    "        # eval the final sc model\n",
    "        input_name = f\"sc_model_{self.iterations}_1\"\n",
    "        output_name = f\"eval_score_{self.iterations}_1\"\n",
    "        chain = self.evaluate_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        input_name = f\"sc_model_{self.iterations}_2\"\n",
    "        output_name = f\"eval_score_{self.iterations}_2\"\n",
    "        chain = self.evaluate_agent(input_name,output_name)\n",
    "        chains.append(chain)\n",
    "        output_keys.append(chain.output_key)\n",
    "\n",
    "        self.overall_chain = SequentialChain(\n",
    "            chains=chains,\n",
    "            input_variables=[\"paper_ref\",\"ref_code\"],\n",
    "            output_variables=output_keys,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    # Run multi-agent system\n",
    "    def Run(self, inputs):\n",
    "        outputs = self.overall_chain(inputs)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"categories\": {\n",
      "    \"harassment\": false,\n",
      "    \"harassment/threatening\": false,\n",
      "    \"hate\": false,\n",
      "    \"hate/threatening\": false,\n",
      "    \"self-harm\": false,\n",
      "    \"self-harm/instructions\": false,\n",
      "    \"self-harm/intent\": false,\n",
      "    \"sexual\": false,\n",
      "    \"sexual/minors\": false,\n",
      "    \"violence\": false,\n",
      "    \"violence/graphic\": false\n",
      "  },\n",
      "  \"category_scores\": {\n",
      "    \"harassment\": 0.00019458516908343881,\n",
      "    \"harassment/threatening\": 6.477488113887375e-06,\n",
      "    \"hate\": 0.003069685073569417,\n",
      "    \"hate/threatening\": 5.001786121283658e-06,\n",
      "    \"self-harm\": 7.029921562207164e-06,\n",
      "    \"self-harm/instructions\": 2.7231806143390713e-06,\n",
      "    \"self-harm/intent\": 9.790189778868807e-07,\n",
      "    \"sexual\": 0.0002446114958729595,\n",
      "    \"sexual/minors\": 3.861287405015901e-05,\n",
      "    \"violence\": 0.00010841531184269115,\n",
      "    \"violence/graphic\": 5.488210081239231e-05\n",
      "  },\n",
      "  \"flagged\": false\n",
      "}\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "The semantic communication model consists of the following components:\n1. Semantic Encoder: Extracts semantic information from the original data and encodes it into semantic features.\n2. Channel Encoder: Encodes and modulates the semantic features for transmission over the physical channel.\n3. Channel Decoder: De-modulates and decodes the received signal to recover the transmitted semantic features.\n4. Semantic Decoder: Understands the received semantic features, infers semantic information, and recovers the original data.\n5. Knowledge Base: A universal knowledge model that helps the semantic encoder and decoder understand and infer semantic information effectively."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# generate sc by LLM enhanced multi-agent system\n",
    "args = configs()\n",
    "ref_code = open(args.code_path,\"r\",encoding=\"utf-8\").read()\n",
    "LL_SC = LLM_enhanced_multi_agent_system_for_SC(args)\n",
    "LL_SC.condensate_agent(args.pdf_path)\n",
    "paper_query = \"Composition of the semantic communication model\"\n",
    "if LL_SC.secure_agent(paper_query):\n",
    "    SC_Components = LL_SC.inference_agent(paper_query)\n",
    "    LL_SC.create_chains()\n",
    "    results = LL_SC.Run({\"paper_ref\":SC_Components,\"ref_code\":ref_code})\n",
    "else:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'The semantic communication model consists of the following components:\\n1. Semantic Encoder: Extracts semantic information from the original data and encodes it into semantic features.\\n2. Channel Encoder: Encodes and modulates the semantic features for transmission over the physical channel.\\n3. Channel Decoder: De-modulates and decodes the received signal to recover the transmitted semantic features.\\n4. Semantic Decoder: Understands the received semantic features, infers semantic information, and recovers the original data.\\n5. Knowledge Base: A universal knowledge model that helps the semantic encoder and decoder understand and infer semantic information effectively.'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['paper_ref']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "'```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\nimport math\\n\\n# Generate random sentences as data A\\ndata_A = [\\n    \"Hello there!\",\\n    \"How are you doing?\",\\n    \"PyTorch is great.\",\\n    \"Machine learning is fascinating.\",\\n    \"Let\\'s build something amazing.\",\\n    \"Semantic communication matters.\",\\n    \"Understanding context is important.\",\\n    \"AI is changing the world.\",\\n    \"Keep learning and growing.\",\\n    \"Innovation drives progress.\"\\n]\\n\\n# Tokenize sentences into words\\nwords = [sentence.split() for sentence in data_A]\\n\\n# Create a vocabulary\\nvocab = list(set(word for sentence in words for word in sentence))\\nvocab_size = len(vocab)\\n\\n# Convert words to unique indices\\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\\nidx_to_word = {idx: word for word, idx in word_to_idx.items()}\\n\\n# Convert sentences to numerical representation\\nnumerical_sentences = [[word_to_idx[word] for word in sentence] for sentence in words]\\n\\n# Semantic Encoder\\nclass SemanticEncoder(nn.Module):\\n    def __init__(self, vocab_size, embedding_size, hidden_size):\\n        super(SemanticEncoder, self).__init__()\\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\\n        self.lstm = nn.LSTM(embedding_size, hidden_size)\\n\\n    def forward(self, input_seq):\\n        embedded = self.embedding(input_seq)\\n        output, (hidden, cell) = self.lstm(embedded)\\n        return output\\n\\n# Channel Encoder and Decoder (Simple Identity Mapping)\\nclass ChannelEncoder(nn.Module):\\n    def __init__(self, hidden_size):\\n        super(ChannelEncoder, self).__init__()\\n        self.identity = nn.Identity()\\n\\n    def forward(self, input_features):\\n        return self.identity(input_features)\\n\\nclass ChannelDecoder(nn.Module):\\n    def __init__(self, hidden_size):\\n        super(ChannelDecoder, self).__init__()\\n        self.identity = nn.Identity()\\n\\n    def forward(self, received_features):\\n        return self.identity(received_features)\\n\\n# Define the physical channel as Additive Gaussian White Noise (AWGN)\\nclass PhysicalChannel(nn.Module):\\n    def __init__(self, snr):\\n        super(PhysicalChannel, self).__init__()\\n        self.snr = snr\\n\\n    def forward(self, x):\\n        noise_power = 10 ** (self.snr / 10)  # Calculate the noise power from the SNR\\n        noise = math.sqrt(noise_power) * torch.randn_like(x)  # Generate Gaussian white noise with the same shape as x\\n        y = x + noise  # Add noise to the signal\\n        return y\\n\\n# Semantic Decoder\\nclass SemanticDecoder(nn.Module):\\n    def __init__(self, hidden_size, vocab_size):\\n        super(SemanticDecoder, self).__init__()\\n        self.lstm = nn.LSTM(hidden_size, hidden_size)\\n        self.linear = nn.Linear(hidden_size, vocab_size)\\n\\n    def forward(self, hidden):\\n        output, _ = self.lstm(hidden)\\n        output = self.linear(output)\\n        return output\\n\\n# Instantiate the model components\\nembedding_size = 64\\nhidden_size = 128\\nsnr = 10  # SNR set to 10 dB\\n\\nsemantic_encoder = SemanticEncoder(vocab_size, embedding_size, hidden_size)\\nchannel_encoder = ChannelEncoder(hidden_size)\\nchannel_decoder = ChannelDecoder(hidden_size)\\nsemantic_decoder = SemanticDecoder(hidden_size, vocab_size)\\nphysical_channel = PhysicalChannel(snr)\\n\\n# Define loss and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(list(semantic_encoder.parameters()) +\\n                       list(channel_encoder.parameters()) +\\n                       list(channel_decoder.parameters()) +\\n                       list(semantic_decoder.parameters()), lr=0.001)\\n\\n# Train the model\\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    total_loss = 0.0\\n    for sentence in numerical_sentences:\\n        optimizer.zero_grad()\\n        input_seq = torch.tensor(sentence)  # Input: all words except the last\\n        target_seq = torch.tensor(sentence)  # Target: all words except the first\\n        semantic_feature = semantic_encoder(input_seq)\\n        encoded_features = channel_encoder(semantic_feature)\\n        received_features = physical_channel(encoded_features)\\n        decoded_features = channel_decoder(received_features)\\n        output = semantic_decoder(decoded_features)\\n\\n        loss = criterion(output, target_seq)\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    if (epoch + 1) % 10 == 0:\\n        print(f\\'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\\')\\n\\n# Test the semantic communication model\\nfor i, sentence in enumerate(numerical_sentences):\\n    test_input = torch.tensor(sentence)\\n    with torch.no_grad():\\n        semantic_feature = semantic_encoder(test_input)\\n        encoded_features = channel_encoder(semantic_feature)\\n        received_features = encoded_features + torch.randn_like(encoded_features) * 0.1\\n        decoded_features = channel_decoder(received_features)\\n        output = semantic_decoder(decoded_features)\\n        predicted_indices = torch.argmax(output, dim=2).squeeze().numpy()\\n        predicted_sentence = \\' \\'.join([idx_to_word[idx] for idx in predicted_indices])\\n\\n        print(\"Original Sentence:\", data_A[i])\\n        print(\"Predicted Sentence:\", predicted_sentence)\\n\\n# Calculate BLEU score for evaluation\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[sentence.split()] for sentence in data_A]\\nhypothesis = [predicted_sentence.split() for predicted_sentence in predicted_sentences]\\n\\nbleu_scores = [sentence_bleu(reference, hypothesis) for reference, hypothesis in zip(reference, hypothesis)]\\naverage_bleu_score = np.mean(bleu_scores)\\nprint(\"Average BLEU Score:\", average_bleu_score)\\n```'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['sc_model_3_2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "'```json\\n{\\n    \"model parameters\": 457042,\\n    \"evaluated score\": 75,\\n    \"evaluation\": \"The Python-based semantic communication model has been implemented with necessary components including Semantic Encoder, Channel Encoder, Channel Decoder, Semantic Decoder, and Physical Channel modules. The model is trained using sentences converted into numerical representations and optimized using the Adam optimizer with CrossEntropyLoss. The BLEU score evaluation indicates reasonable performance in generating predicted sentences. The advantage of the generated code is the comprehensive implementation of the model components and training process. However, the disadvantage is the relatively high number of model parameters, exceeding the limitation set for this evaluation. Overall, the model demonstrates good quality and performance for semantic communication tasks.\"\\n}\\n```'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['eval_score_3_2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
